{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "315a01e9",
   "metadata": {
    "papermill": {
     "duration": 0.002474,
     "end_time": "2026-01-08T22:28:45.381727",
     "exception": false,
     "start_time": "2026-01-08T22:28:45.379253",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Cell 1: Environment & Hyper-Parameters\n",
    "Goal: Set high-resolution inputs and sensitive detection thresholds to jump from 0.321 to the Top 50."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6c2df8b7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-08T22:28:45.386275Z",
     "iopub.status.busy": "2026-01-08T22:28:45.385991Z",
     "iopub.status.idle": "2026-01-08T22:29:15.436941Z",
     "shell.execute_reply": "2026-01-08T22:29:15.436191Z"
    },
    "papermill": {
     "duration": 30.056544,
     "end_time": "2026-01-08T22:29:15.439971",
     "exception": false,
     "start_time": "2026-01-08T22:28:45.383427",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-01-08 22:28:59.946737: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1767911340.146313      24 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1767911340.203303      24 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "W0000 00:00:1767911340.683904      24 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1767911340.683946      24 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1767911340.683949      24 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1767911340.683951      24 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Environment Optimized. Resolution: 616px\n"
     ]
    }
   ],
   "source": [
    "import os, cv2, json, math, torch, random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from pathlib import Path\n",
    "from PIL import Image\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from transformers import AutoImageProcessor, AutoModel\n",
    "\n",
    "class CFG:\n",
    "    seed = 42\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    \n",
    "    # P100 is powerful enough to handle 616px efficiently\n",
    "    img_size = 616 \n",
    "    \n",
    "    # Paths (Verified based on your last screenshot)\n",
    "    test_dir = \"/kaggle/input/recodai-luc-scientific-image-forgery-detection/test_images\"\n",
    "    sample_sub = \"/kaggle/input/recodai-luc-scientific-image-forgery-detection/sample_submission.csv\"\n",
    "    dino_path = \"/kaggle/input/dinov2/pytorch/base/1\"\n",
    "    model_weights = \"/kaggle/input/cnndinov2-pbd/CNNDINOv2-U52/CNNDINOv2-U52/model_seg_final.pt\"\n",
    "    \n",
    "    # Aggressive Winning Thresholds\n",
    "    area_thr = 180    \n",
    "    mean_thr = 0.20   \n",
    "    use_tta = True\n",
    "\n",
    "def seed_everything(seed):\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "\n",
    "seed_everything(CFG.seed)\n",
    "print(f\"Environment Optimized. Resolution: {CFG.img_size}px\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df7aa628",
   "metadata": {
    "papermill": {
     "duration": 0.001606,
     "end_time": "2026-01-08T22:29:15.443186",
     "exception": false,
     "start_time": "2026-01-08T22:29:15.441580",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Cell 2: Advanced Model Architecture\n",
    "Goal: Reconstruct the spatial features from DINOv2 embeddings using a multi-scale decoder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6c456835",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-08T22:29:15.448301Z",
     "iopub.status.busy": "2026-01-08T22:29:15.447159Z",
     "iopub.status.idle": "2026-01-08T22:29:15.456530Z",
     "shell.execute_reply": "2026-01-08T22:29:15.456032Z"
    },
    "papermill": {
     "duration": 0.013112,
     "end_time": "2026-01-08T22:29:15.457874",
     "exception": false,
     "start_time": "2026-01-08T22:29:15.444762",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class DinoTinyDecoder(nn.Module):\n",
    "    def __init__(self, in_ch=768, out_ch=1):\n",
    "        super().__init__()\n",
    "        # Matches your trained weights structure\n",
    "        self.block1 = nn.Sequential(nn.Conv2d(in_ch, 384, 3, padding=1), nn.ReLU(inplace=True))\n",
    "        self.block2 = nn.Sequential(nn.Conv2d(384, 192, 3, padding=1), nn.ReLU(inplace=True))\n",
    "        self.block3 = nn.Sequential(nn.Conv2d(192, 96, 3, padding=1), nn.ReLU(inplace=True))\n",
    "        self.conv_out = nn.Conv2d(96, out_ch, kernel_size=1)\n",
    "    \n",
    "    def forward(self, f, target_size):\n",
    "        # Gradual upsampling to recover pixel-level edges\n",
    "        x = F.interpolate(self.block1(f), size=(88, 88), mode='bilinear', align_corners=False)\n",
    "        x = F.interpolate(self.block2(x), size=(176, 176), mode='bilinear', align_corners=False)\n",
    "        x = F.interpolate(self.block3(x), size=(352, 352), mode='bilinear', align_corners=False)\n",
    "        x = self.conv_out(x)\n",
    "        return F.interpolate(x, size=target_size, mode='bilinear', align_corners=False)\n",
    "\n",
    "class ScientificForgeryModel(nn.Module):\n",
    "    def __init__(self, dino_path):\n",
    "        super().__init__()\n",
    "        self.processor = AutoImageProcessor.from_pretrained(dino_path, local_files_only=True)\n",
    "        self.encoder = AutoModel.from_pretrained(dino_path, local_files_only=True)\n",
    "        for p in self.encoder.parameters(): p.requires_grad = False\n",
    "        self.seg_head = DinoTinyDecoder(768, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        imgs = (x * 255).clamp(0, 255).byte().permute(0, 2, 3, 1).cpu().numpy()\n",
    "        inputs = self.processor(images=list(imgs), return_tensors=\"pt\").to(CFG.device)\n",
    "        with torch.no_grad():\n",
    "            feats = self.encoder(**inputs).last_hidden_state\n",
    "        B, N, C = feats.shape\n",
    "        s = int(math.sqrt(N-1))\n",
    "        fmap = feats[:, 1:, :].permute(0, 2, 1).reshape(B, C, s, s)\n",
    "        return self.seg_head(fmap, (CFG.img_size, CFG.img_size))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dda6df91",
   "metadata": {
    "papermill": {
     "duration": 0.001606,
     "end_time": "2026-01-08T22:29:15.461053",
     "exception": false,
     "start_time": "2026-01-08T22:29:15.459447",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Cell 3: Post-Processing & Encoding\n",
    "Goal: Clean output noise and convert masks to RLE format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "38bc2e5e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-08T22:29:15.465494Z",
     "iopub.status.busy": "2026-01-08T22:29:15.464972Z",
     "iopub.status.idle": "2026-01-08T22:29:15.470564Z",
     "shell.execute_reply": "2026-01-08T22:29:15.469891Z"
    },
    "papermill": {
     "duration": 0.009413,
     "end_time": "2026-01-08T22:29:15.472014",
     "exception": false,
     "start_time": "2026-01-08T22:29:15.462601",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def rle_encode(mask):\n",
    "    pixels = mask.T.flatten()\n",
    "    dots = np.where(pixels == 1)[0]\n",
    "    if len(dots) == 0: return \"authentic\"\n",
    "    run_lengths = []\n",
    "    prev = -2\n",
    "    for b in dots:\n",
    "        if b > prev + 1: run_lengths.extend((b + 1, 0))\n",
    "        run_lengths[-1] += 1\n",
    "        prev = b\n",
    "    return json.dumps([int(x) for x in run_lengths])\n",
    "\n",
    "def post_process(prob, original_size):\n",
    "    # Gaussian Blur to smooth out the probability map\n",
    "    prob_refined = cv2.GaussianBlur(prob, (5, 5), 0)\n",
    "    # Adaptive Thresholding: Mean + std deviation for better localization\n",
    "    thr = np.mean(prob_refined) + 0.3 * np.std(prob_refined)\n",
    "    mask = (prob_refined > thr).astype(np.uint8)\n",
    "    \n",
    "    # Morphological closing to join fragmented forgery segments\n",
    "    kernel = np.ones((5,5), np.uint8)\n",
    "    mask = cv2.morphologyEx(mask, cv2.MORPH_CLOSE, kernel)\n",
    "    mask = cv2.resize(mask, original_size, interpolation=cv2.INTER_NEAREST)\n",
    "    return mask"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f53166e",
   "metadata": {
    "papermill": {
     "duration": 0.001543,
     "end_time": "2026-01-08T22:29:15.475203",
     "exception": false,
     "start_time": "2026-01-08T22:29:15.473660",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Cell 4: Final Inference Loop\n",
    "Goal: Generate the submission file with strict type-safety."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "95fa82ea",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-08T22:29:15.479825Z",
     "iopub.status.busy": "2026-01-08T22:29:15.479121Z",
     "iopub.status.idle": "2026-01-08T22:29:21.791538Z",
     "shell.execute_reply": "2026-01-08T22:29:21.790678Z"
    },
    "papermill": {
     "duration": 6.316267,
     "end_time": "2026-01-08T22:29:21.793053",
     "exception": false,
     "start_time": "2026-01-08T22:29:15.476786",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.52, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Professional Weights Loaded Successfully ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "High-Res Processing: 100%|██████████| 1/1 [00:00<00:00,  1.23it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- SUCCESS: Optimized submission.csv generated! ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "def run_optimized_inference():\n",
    "    model = ScientificForgeryModel(CFG.dino_path).to(CFG.device)\n",
    "    if os.path.exists(CFG.model_weights):\n",
    "        state_dict = torch.load(CFG.model_weights, map_location=CFG.device)\n",
    "        model.load_state_dict(state_dict, strict=False)\n",
    "        print(\"--- Professional Weights Loaded Successfully ---\")\n",
    "    \n",
    "    model.eval()\n",
    "    results = []\n",
    "    test_images = sorted(os.listdir(CFG.test_dir))\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for img_name in tqdm(test_images, desc=\"High-Res Processing\"):\n",
    "            img_path = os.path.join(CFG.test_dir, img_name)\n",
    "            pil_img = Image.open(img_path).convert(\"RGB\")\n",
    "            orig_w, orig_h = pil_img.size\n",
    "            \n",
    "            img_t = torch.from_numpy(np.array(pil_img.resize((CFG.img_size, CFG.img_size)), np.float32)/255.).permute(2,0,1)[None].to(CFG.device)\n",
    "            \n",
    "            logits = model(img_t)\n",
    "            if CFG.use_tta:\n",
    "                # TTA: Horizontal Flip\n",
    "                logits_h = model(torch.flip(img_t, dims=[3]))\n",
    "                logits = (logits + torch.flip(logits_h, dims=[3])) / 2.0\n",
    "            \n",
    "            prob = torch.sigmoid(logits)[0,0].cpu().numpy()\n",
    "            mask = post_process(prob, (orig_w, orig_h))\n",
    "            \n",
    "            # Area and Mean-Confidence validation\n",
    "            area = mask.sum()\n",
    "            mask_s = cv2.resize(mask, (CFG.img_size, CFG.img_size), interpolation=cv2.INTER_NEAREST)\n",
    "            mean_c = prob[mask_s == 1].mean() if area > 0 else 0\n",
    "            \n",
    "            if area >= CFG.area_thr and mean_c >= CFG.mean_thr:\n",
    "                annotation = rle_encode(mask)\n",
    "            else:\n",
    "                annotation = \"authentic\"\n",
    "            \n",
    "            results.append({\"case_id\": str(Path(img_name).stem), \"annotation\": annotation})\n",
    "\n",
    "    sub_df = pd.DataFrame(results)\n",
    "    sample_df = pd.read_csv(CFG.sample_sub)\n",
    "    sample_df['case_id'] = sample_df['case_id'].astype(str)\n",
    "    sub_df['case_id'] = sub_df['case_id'].astype(str)\n",
    "    \n",
    "    final_sub = sample_df[['case_id']].merge(sub_df, on=\"case_id\", how=\"left\").fillna(\"authentic\")\n",
    "    final_sub.to_csv(\"submission.csv\", index=False)\n",
    "    print(\"--- SUCCESS: Optimized submission.csv generated! ---\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    run_optimized_inference()"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "databundleVersionId": 14878066,
     "isSourceIdPinned": false,
     "sourceId": 113558,
     "sourceType": "competition"
    },
    {
     "datasetId": 9153851,
     "sourceId": 14407528,
     "sourceType": "datasetVersion"
    },
    {
     "sourceId": 290828862,
     "sourceType": "kernelVersion"
    },
    {
     "isSourceIdPinned": false,
     "modelId": 986,
     "modelInstanceId": 3326,
     "sourceId": 4534,
     "sourceType": "modelInstanceVersion"
    }
   ],
   "dockerImageVersionId": 31240,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 42.111114,
   "end_time": "2026-01-08T22:29:24.934708",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2026-01-08T22:28:42.823594",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
