{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.12.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"tpuV5e8","dataSources":[{"sourceId":119261,"databundleVersionId":14363498,"sourceType":"competition"}],"dockerImageVersionId":31235,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"source":"<a href=\"https://www.kaggle.com/code/mr0106/google-tunix-hack?scriptVersionId=289114436\" target=\"_blank\"><img align=\"left\" alt=\"Kaggle\" title=\"Open in Kaggle\" src=\"https://kaggle.com/static/images/open-in-kaggle.svg\"></a>","metadata":{},"cell_type":"markdown"},{"cell_type":"markdown","source":"C","metadata":{}},{"cell_type":"code","source":"# STEP 1: Installation and Environment Setup (Run this first)\n# This clears old libraries and installs the specific versions needed for Gemma 3 on TPU.\n\n# 1. Purge cache and remove conflicting versions\n!pip cache purge\n!pip uninstall -y jax jaxlib libtpu-tpuv3 numpy ml-dtypes orbax-checkpoint flax google-tunix qwix\n\n# 2. Install the optimized TPU stack and competition-specific libraries\n!pip install -q \"jax[tpu]==0.5.1\" -f https://storage.googleapis.com/jax-releases/libtpu_releases.html\n!pip install -q \"numpy<2.0.0\" \"ml-dtypes>=0.5.0\" \"flax==0.12.2\" \"orbax-checkpoint==0.6.4\"\n!pip install -q \"google-tunix[prod]==0.1.3\" git+https://github.com/google/qwix.git\n!pip install -q safetensors\nprint(\"\\nâœ… SETUP SUCCESSFUL! Now click the 'RESTART SESSION' button in the toolbar.\")","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import pandas as pd\nimport os, jax, jax.numpy as jnp\nimport flax.nnx as nnx\n\n# --- 1. ÙˆØ¸Ø§Ø¦Ù Ø§Ù„ØªÙˆÙ„ÙŠØ¯ Ø§Ù„Ø°ÙƒÙŠØ© (Gemma 3 Optimized) ---\n\ndef create_gemma3_mask(tokens):\n    \"\"\"Ø¥Ù†Ø´Ø§Ø¡ Ù‚Ù†Ø§Ø¹ Ø±Ø¨Ø§Ø¹ÙŠ Ø§Ù„Ø£Ø¨Ø¹Ø§Ø¯ (Batch, Heads, Seq, Seq) Ù„ØªØ¬Ù†Ø¨ Ø®Ø·Ø£ Unpack\"\"\"\n    seq_len = tokens.shape[1]\n    # Ø§Ù„Ù‚Ù†Ø§Ø¹ Ø§Ù„Ø³Ø¨Ø¨ÙŠ: ÙŠÙ…Ù†Ø¹ Ø§Ù„Ù…ÙˆØ¯ÙŠÙ„ Ù…Ù† Ø±Ø¤ÙŠØ© Ø§Ù„Ù…Ø³ØªÙ‚Ø¨Ù„\n    mask = jnp.tril(jnp.ones((seq_len, seq_len)))\n    # ØªÙ…Ø¯ÙŠØ¯ Ø§Ù„Ø£Ø¨Ø¹Ø§Ø¯ Ù„ØªÙ†Ø§Ø³Ø¨ (Batch=1, Heads=1, Seq, Seq)\n    return mask[None, None, :, :].astype(jnp.bool_)\n\n@nnx.jit\ndef predict_step(model, tokens):\n    \"\"\"Ø®Ø·ÙˆØ© Ø§Ù„ØªÙ†Ø¨Ø¤ Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… JIT Ù„Ø¶Ù…Ø§Ù† Ø§Ù„Ø³Ø±Ø¹Ø© Ø§Ù„Ù‚ØµÙˆÙ‰ Ø¹Ù„Ù‰ TPU\"\"\"\n    seq_len = tokens.shape[1]\n    positions = jnp.arange(seq_len)[None, :]\n    mask = create_gemma3_mask(tokens)\n    \n    # Ø§Ø³ØªØ¯Ø¹Ø§Ø¡ Ø§Ù„Ù…ÙˆØ¯ÙŠÙ„ ÙˆØªÙ„Ù‚ÙŠ Ø§Ù„Ù…Ø®Ø±Ø¬Ø§Øª (ØªØ¬Ø§ÙˆØ² Ø®Ø·Ø£ Ø§Ù„Ù€ 4 Ù…Ø®Ø±Ø¬Ø§Øª)\n    outputs = model(tokens, positions=positions, attention_mask=mask, cache=None)\n    \n    # Ù…Ø®Ø±Ø¬Ø§Øª Gemma 3 ÙÙŠ Tunix Ù‡ÙŠ TupleØŒ Ø§Ù„Ø¹Ù†ØµØ± Ø§Ù„Ø£ÙˆÙ„ Ù‡Ùˆ Ø§Ù„Ù€ Logits\n    logits = outputs[0] if isinstance(outputs, (list, tuple)) else outputs\n    return jnp.argmax(logits[:, -1, :], axis=-1)\n\ndef generate_answer(model, tokenizer, prompt, max_new=128):\n    \"\"\"Ø­Ù„Ù‚Ø© Ø§Ù„ØªÙˆÙ„ÙŠØ¯: ØªØ­ÙˆÙŠÙ„ Ø§Ù„Ù†ØµØŒ Ø§Ù„ØªÙ†Ø¨Ø¤ØŒ Ø«Ù… ÙÙƒ Ø§Ù„ØªØ´ÙÙŠØ±\"\"\"\n    curr_tokens = tokenizer(prompt, return_tensors=\"jax\").input_ids\n    \n    for _ in range(max_new):\n        next_t = predict_step(model, curr_tokens)\n        # Ø¯Ù…Ø¬ Ø§Ù„ØªÙˆÙƒÙ† Ø§Ù„Ø¬Ø¯ÙŠØ¯ Ù…Ø¹ Ø§Ù„Ø­ÙØ§Ø¸ Ø¹Ù„Ù‰ Ø´ÙƒÙ„ Ø§Ù„Ù…ØµÙÙˆÙØ© (1, N)\n        curr_tokens = jnp.concatenate([curr_tokens, next_t.reshape(1, 1)], axis=1)\n        if next_t[0] == tokenizer.eos_token_id:\n            break\n            \n    full_text = tokenizer.decode(curr_tokens[0], skip_special_tokens=True)\n    # Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø¬Ø²Ø¡ Ø§Ù„Ù…ÙˆØ¯ÙŠÙ„ ÙÙ‚Ø· Ù…Ù† Ø§Ù„Ù€ Prompt\n    return full_text.split(\"model\")[-1].strip() if \"model\" in full_text else full_text\n\n# --- 2. Ø§Ù„ØªÙ†ÙÙŠØ° (Inference & Submission) ---\n\n# Ø§Ù„Ø¨Ø­Ø« Ø§Ù„Ø¯ÙŠÙ†Ø§Ù…ÙŠÙƒÙŠ Ø¹Ù† Ù…Ù„Ù Ø§Ù„Ù…Ø³Ø§Ø¨Ù‚Ø©\ninput_file = next((os.path.join(r, f) for r, d, fs in os.walk('/kaggle/input') \n                   if any(f.endswith(x) for x in ['test.csv'])), None)\n\nif not input_file:\n    print(\"âš ï¸ Ù…Ù„Ù Ø§Ù„Ù…Ø³Ø§Ø¨Ù‚Ø© ØºÙŠØ± Ù…ÙˆØ¬ÙˆØ¯! ØªØ£ÙƒØ¯ Ù…Ù† Ø¶ØºØ· 'Add Input' ÙˆØ¥Ø¶Ø§ÙØ© Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ù…Ø³Ø§Ø¨Ù‚Ø©.\")\n    test_df = pd.DataFrame({'id': [0], 'question': [\"Solve: 2x + 5 = 15\"]})\nelse:\n    test_df = pd.read_csv(input_file)\n\nprint(f\"ğŸš€ Ø¨Ø¯Ø¡ Ø§Ù„ØªÙˆÙ„ÙŠØ¯ Ø¹Ù„Ù‰ TPU Ù„Ù€ {len(test_df)} Ø³Ø¤Ø§Ù„...\")\nresults = []\n\ntry:\n    for i, row in test_df.iterrows():\n        # ØªÙ†Ø³ÙŠÙ‚ Gemma 3 Ø§Ù„Ø±Ø³Ù…ÙŠ Ù„Ù„Ø­ÙˆØ§Ø±\n        prompt = f\"<start_of_turn>user\\n{row['question']}\\nShow your work step by step.<end_of_turn>\\n<start_of_turn>model\\n\"\n        \n        with mesh: # Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ù€ Mesh Ø§Ù„Ù…Ø¹Ø±Ù Ù…Ø³Ø¨Ù‚Ø§Ù‹ Ù„ØªÙˆØ²ÙŠØ¹ Ø§Ù„Ø¹Ù…Ù„ Ø¹Ù„Ù‰ TPU\n            ans = generate_answer(model, tokenizer, prompt)\n            results.append(ans)\n        \n        if (i+1) % 5 == 0 or i == 0:\n            print(f\"âœ… ØªÙ… Ù…Ø¹Ø§Ù„Ø¬Ø© {i+1}/{len(test_df)}\")\n\n    # Ø­ÙØ¸ Ø§Ù„Ù†ØªÙŠØ¬Ø© Ø§Ù„Ù†Ù‡Ø§Ø¦ÙŠØ©\n    submission = pd.DataFrame({'id': test_df['id'], 'answer': results})\n    submission.to_csv('submission.csv', index=False)\n    print(\"\\nâœ¨ ØªÙ… Ø¥Ù†Ø´Ø§Ø¡ submission.csv Ø¨Ù†Ø¬Ø§Ø­! Ø¬Ø§Ù‡Ø² Ù„Ù„ØªØ³Ù„ÙŠÙ… Ø§Ù„Ø¢Ù†.\")\n\nexcept Exception as e:\n    print(f\"âŒ Ø®Ø·Ø£ ØªÙ‚Ù†ÙŠ: {e}\")","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}